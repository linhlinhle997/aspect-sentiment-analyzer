{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Build Dataset","metadata":{}},{"cell_type":"code","source":"!pip install -q datasets==3.2.0\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"thainq107/abte-restaurants\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-13T04:58:45.190869Z","iopub.execute_input":"2025-02-13T04:58:45.191458Z","iopub.status.idle":"2025-02-13T04:58:51.840999Z","shell.execute_reply.started":"2025-02-13T04:58:45.191405Z","shell.execute_reply":"2025-02-13T04:58:51.839965Z"}},"outputs":[],"execution_count":65},{"cell_type":"markdown","source":"## Tokenization","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n\ndef tokenize_and_align_labels(examples):\n    sentences, sentence_tags = [], []\n    labels = []\n\n    for tokens, pols in zip(examples['Tokens'], examples['Polarities']):\n        # Clean and split tokens and tags\n        tokens = str(tokens).replace(\"â€™\", \"\").strip(\"][\").split(\", \")\n        pols = [pol.strip(\"'\\\"\") for pol in str(pols).strip(\"][\").split(\", \")]\n\n        bert_tokens, bert_att = [], []\n        pols_label = 0\n\n        for i in range(len(tokens)):\n            t = tokenizer.tokenize(tokens[i])  # Tokenize each token\n            bert_tokens += t\n            if int(pols[i]) != -1:  # Only add tokens with a valid polarity\n                bert_att += t  # Add tokens to sentence_tags\n                pols_label = int(pols[i])  # Use the last valid polarity as the label\n\n        # Filter sentence_tags to only include unique aspect words\n        sentence_tags.append(\" \".join(set(bert_att)))  # Remove duplicates\n        sentences.append(\" \".join(bert_tokens))  # Full tokenized sentence\n        labels.append(pols_label)  # Final polarity label\n\n    # Tokenize and prepare tensors with padding and truncation\n    tokenized_inputs = tokenizer(sentences, sentence_tags, padding=True, truncation=True, return_tensors=\"pt\")\n    tokenized_inputs[\"labels\"] = labels\n    return tokenized_inputs\n\n# Apply the tokenization and label alignment to the dataset\npreprocessed_ds = ds.map(tokenize_and_align_labels, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T04:58:51.842669Z","iopub.execute_input":"2025-02-13T04:58:51.843100Z","iopub.status.idle":"2025-02-13T04:58:58.473933Z","shell.execute_reply.started":"2025-02-13T04:58:51.843047Z","shell.execute_reply":"2025-02-13T04:58:58.472588Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3602 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"779c1bc2b8d44e43877dc354acfe3c8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1119 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91ddff4a43a74b4394dd46daedd576ec"}},"metadata":{}}],"execution_count":66},{"cell_type":"code","source":"preprocessed_ds.column_names","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T04:58:58.476628Z","iopub.execute_input":"2025-02-13T04:58:58.476996Z","iopub.status.idle":"2025-02-13T04:58:58.483563Z","shell.execute_reply.started":"2025-02-13T04:58:58.476961Z","shell.execute_reply":"2025-02-13T04:58:58.482548Z"}},"outputs":[{"execution_count":67,"output_type":"execute_result","data":{"text/plain":"{'train': ['Tokens',\n  'Tags',\n  'Polarities',\n  'input_ids',\n  'attention_mask',\n  'labels'],\n 'test': ['Tokens',\n  'Tags',\n  'Polarities',\n  'input_ids',\n  'attention_mask',\n  'labels']}"},"metadata":{}}],"execution_count":67},{"cell_type":"code","source":"print(\"Tokens:\", preprocessed_ds['train']['Tokens'][0])\nprint(\"Tags:\", preprocessed_ds['train']['Tags'][0])\nprint(\"Polarities:\", preprocessed_ds['train']['Polarities'][0])\nprint(\"Input IDs:\", preprocessed_ds['train']['input_ids'][0])\nprint(\"Attention Mask:\", preprocessed_ds['train']['attention_mask'][0])\nprint(\"Labels:\", preprocessed_ds['train']['labels'][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T04:58:58.485504Z","iopub.execute_input":"2025-02-13T04:58:58.485830Z","iopub.status.idle":"2025-02-13T04:58:59.560398Z","shell.execute_reply.started":"2025-02-13T04:58:58.485776Z","shell.execute_reply":"2025-02-13T04:58:59.559269Z"}},"outputs":[{"name":"stdout","text":"Tokens: ['But', 'the', 'staff', 'was', 'so', 'horrible', 'to', 'us', '.']\nTags: ['0', '0', '1', '0', '0', '0', '0', '0', '0']\nPolarities: ['-1', '-1', '0', '-1', '-1', '-1', '-1', '-1', '-1']\nInput IDs: [101, 1005, 2021, 1005, 1005, 1996, 1005, 1005, 3095, 1005, 1005, 2001, 1005, 1005, 2061, 1005, 1005, 9202, 1005, 1005, 2000, 1005, 1005, 2149, 1005, 1005, 1012, 1005, 102, 1005, 3095, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nAttention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nLabels: 0\n","output_type":"stream"}],"execution_count":68},{"cell_type":"markdown","source":"## Evaluate","metadata":{}},{"cell_type":"code","source":"!pip install -q evaluate==0.4.3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T04:58:59.561479Z","iopub.execute_input":"2025-02-13T04:58:59.561923Z","iopub.status.idle":"2025-02-13T04:59:04.613867Z","shell.execute_reply.started":"2025-02-13T04:58:59.561883Z","shell.execute_reply":"2025-02-13T04:59:04.612361Z"}},"outputs":[],"execution_count":69},{"cell_type":"code","source":"import numpy as np\nimport evaluate\n\naccuracy = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    return accuracy.compute(predictions=predictions, references=labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T04:59:04.615330Z","iopub.execute_input":"2025-02-13T04:59:04.615707Z","iopub.status.idle":"2025-02-13T04:59:04.900620Z","shell.execute_reply.started":"2025-02-13T04:59:04.615671Z","shell.execute_reply":"2025-02-13T04:59:04.899558Z"}},"outputs":[],"execution_count":70},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\nid2label = {\n    0: \"Negative\",\n    1: \"Neutral\",\n    2: \"Positive\"\n}\nlabel2id = {\n    \"Negative\": 0,\n    \"Neutral\": 1,\n    \"Positive\": 2\n}\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"distilbert/distilbert-base-uncased\",\n    num_labels=3, id2label=id2label, label2id=label2id\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T04:59:04.901950Z","iopub.execute_input":"2025-02-13T04:59:04.902339Z","iopub.status.idle":"2025-02-13T04:59:04.999144Z","shell.execute_reply.started":"2025-02-13T04:59:04.902308Z","shell.execute_reply":"2025-02-13T04:59:04.998130Z"}},"outputs":[{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":71},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"import os\n\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T04:59:05.000319Z","iopub.execute_input":"2025-02-13T04:59:05.000658Z","iopub.status.idle":"2025-02-13T04:59:05.005545Z","shell.execute_reply.started":"2025-02-13T04:59:05.000627Z","shell.execute_reply":"2025-02-13T04:59:05.004582Z"}},"outputs":[],"execution_count":72},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\ntraining_args = TrainingArguments(\n    output_dir=\"abte-restaurants-distilbert-base-uncased\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=5,\n    weight_decay=0.01,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=preprocessed_ds[\"train\"],\n    eval_dataset=preprocessed_ds[\"test\"],\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T04:59:05.008823Z","iopub.execute_input":"2025-02-13T04:59:05.009216Z","iopub.status.idle":"2025-02-13T05:04:35.481459Z","shell.execute_reply.started":"2025-02-13T04:59:05.009182Z","shell.execute_reply":"2025-02-13T05:04:35.480072Z"}},"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n<ipython-input-73-e53a450163f0>:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='565' max='565' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [565/565 05:28, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.606808</td>\n      <td>0.756032</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.554166</td>\n      <td>0.778374</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.518584</td>\n      <td>0.793566</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>0.500031</td>\n      <td>0.809651</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.569200</td>\n      <td>0.515907</td>\n      <td>0.807864</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":73,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=565, training_loss=0.5443617018978153, metrics={'train_runtime': 329.331, 'train_samples_per_second': 54.687, 'train_steps_per_second': 1.716, 'total_flos': 1369959836789880.0, 'train_loss': 0.5443617018978153, 'epoch': 5.0})"},"metadata":{}}],"execution_count":73},{"cell_type":"code","source":"trainer.save_model(\"abte-restaurants-distilbert-base-uncased\")\ntokenizer.save_pretrained(\"abte-restaurants-distilbert-base-uncased\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T05:04:35.483524Z","iopub.execute_input":"2025-02-13T05:04:35.484040Z","iopub.status.idle":"2025-02-13T05:04:36.816679Z","shell.execute_reply.started":"2025-02-13T05:04:35.484000Z","shell.execute_reply":"2025-02-13T05:04:36.815470Z"}},"outputs":[{"execution_count":74,"output_type":"execute_result","data":{"text/plain":"('abte-restaurants-distilbert-base-uncased/tokenizer_config.json',\n 'abte-restaurants-distilbert-base-uncased/special_tokens_map.json',\n 'abte-restaurants-distilbert-base-uncased/vocab.txt',\n 'abte-restaurants-distilbert-base-uncased/added_tokens.json',\n 'abte-restaurants-distilbert-base-uncased/tokenizer.json')"},"metadata":{}}],"execution_count":74},{"cell_type":"markdown","source":"## Prediction","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\n\ntoken_classifier = pipeline(\n    \"ner\",\n    model=\"abte-restaurants-distilbert-base-uncased\",\n    tokenizer=\"abte-restaurants-distilbert-base-uncased\",\n    aggregation_strategy=\"simple\"  # This returns individual tokens\n)\n\nclassifier = pipeline(\n    \"text-classification\",\n    model=\"abte-restaurants-distilbert-base-uncased\",\n    tokenizer=\"abte-restaurants-distilbert-base-uncased\",\n)\n\n\ntest_sentence = \"The bread is top notch as well\"\nresults = token_classifier(test_sentence)\n\n# Joining recognized words from the NER pipeline\nsentence_tags = \" \".join([result[\"word\"] for result in results])\n\n# Classifying the sentence with additional sentence tags\npred_label = classifier(f\"{test_sentence} [SEP] {sentence_tags}\")\n\nsentence_tags, pred_label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T05:05:15.908068Z","iopub.execute_input":"2025-02-13T05:05:15.908541Z","iopub.status.idle":"2025-02-13T05:05:16.349352Z","shell.execute_reply.started":"2025-02-13T05:05:15.908505Z","shell.execute_reply":"2025-02-13T05:05:16.347846Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\nDevice set to use cuda:0\n","output_type":"stream"},{"execution_count":78,"output_type":"execute_result","data":{"text/plain":"('the bread is top notch as well',\n [{'label': 'Positive', 'score': 0.9333567023277283}])"},"metadata":{}}],"execution_count":78},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}